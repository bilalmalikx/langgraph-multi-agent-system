{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80b133bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cff8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f758e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class jokeState(TypedDict):\n",
    "    topic: str \n",
    "    joke: str \n",
    "    explanation: str "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "322834c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_joke(state: jokeState):\n",
    "    prompt = f'generate a joke on the topic {state[\"topic\"]}'\n",
    "    response = llm.invoke(prompt).content\n",
    "\n",
    "    return {'joke': response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4e75527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_explanation(state: jokeState):\n",
    "    prompt = f'write an explanation for the joke - {state[\"joke\"]}'\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return {'explanation': response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39b78157",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = StateGraph(jokeState)\n",
    "\n",
    "state.add_node('generate_joke', generate_joke)\n",
    "state.add_node('generate_explanation', generate_explanation)\n",
    "\n",
    "state.add_edge(START, 'generate_joke')\n",
    "state.add_edge('generate_joke', 'generate_explanation')\n",
    "state.add_edge('generate_explanation', END)\n",
    "\n",
    "workflow = state.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1d6bd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Create a joke about monkey',\n",
       " 'joke': 'Why did the monkey like to hang out with the banana?\\n\\nBecause they were both peeling good!',\n",
       " 'explanation': AIMessage(content='This joke is a play on words, using the pun of \"peeling\" to mean both feeling good and the act of peeling a banana. In this case, the monkey enjoys hanging out with the banana because they are both \"peeling good\" in the sense that they are feeling good and also physically peeling, or removing the skin from, the banana. It\\'s a light-hearted and silly joke that plays on the similarities between the monkey and the banana.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 94, 'prompt_tokens': 34, 'total_tokens': 128, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-DESszofG5FBjbjYlnsfL20mg68Dm1', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019ca7b2-3722-7b21-8fa8-1afe455b6f0c-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 34, 'output_tokens': 94, 'total_tokens': 128, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state = {\n",
    "    'topic': \"Create a joke about monkey\"\n",
    "}\n",
    "\n",
    "workflow.invoke(initial_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
